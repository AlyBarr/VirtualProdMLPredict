{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b585fa19-1a8c-4f1e-9895-d23eb87ecf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully yay!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 1. IMPORT REQUIRED LIBRARIES\n",
    "# -------------------------------------------------------------\n",
    "# These libraries handle:\n",
    "# - file reading (os, glob, Path)\n",
    "# - numerical and data manipulation (numpy, pandas)\n",
    "# - machine learning preprocessing (scikit-learn)\n",
    "# - model building and saving (pickle)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "import os, re, glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"Libraries imported successfully yay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98b134f-2950-4083-99a5-69c2b7906427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 180\n",
      "['CleanedKARDDataset\\\\RealWorldCoordinates\\\\a01_s01_e01_realworld.csv', 'CleanedKARDDataset\\\\RealWorldCoordinates\\\\a01_s02_e01_realworld.csv', 'CleanedKARDDataset\\\\RealWorldCoordinates\\\\a01_s03_e01_realworld.csv']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 2. LOAD AND VERIFY DATASET\n",
    "# -------------------------------------------------------------\n",
    "# The dataset comes from the CleanedKARDDataset ZIP you uploaded.\n",
    "# Each CSV file represents one action clip containing 3D joint positions\n",
    "# in real-world coordinates (x, y, z) for different body joints.\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Change this path if your folder is stored elsewhere\n",
    "DATA_DIR = Path(\"CleanedKARDDataset/RealWorldCoordinates\")  # change if needed\n",
    "\n",
    "# Make sure the folder exists before continuing\n",
    "assert DATA_DIR.exists(), f\"Data dir not found: {DATA_DIR}\"\n",
    "\n",
    "\n",
    "# Collect all CSV files in the folder\n",
    "files = sorted(glob.glob(str(DATA_DIR / \"*.csv\")))\n",
    "print(\"Found files:\", len(files))\n",
    "\n",
    "# Preview first few file names\n",
    "print(files[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0faff85f-8257-42f6-a6d6-c33ea9b143f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 3. PARSE SEQUENCES INTO ARRAYS\n",
    "# -------------------------------------------------------------\n",
    "# Each CSV lists joint coordinates for multiple frames.\n",
    "# Reconstructing the frames so each file becomes:\n",
    "#     seq shape = (T, J, 3)\n",
    "# where:\n",
    "#   T = number of frames\n",
    "#   J = number of joints\n",
    "#   3 = x, y, z coordinates\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def parse_sequence(csv_path: str):\n",
    "    \"\"\"\n",
    "    Convert a single CSV into a 3D numpy array [frames, joints, coords].\n",
    "    'Head' rows are used to detect new frames in the CSV stream.\n",
    "    Returns:\n",
    "        seq  -> np.ndarray of shape (T, J, 3)\n",
    "        joint_order -> list of joint names in consistent order\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    frames = []     # stores frame DataFrames\n",
    "    current = []    # collects rows for current frame\n",
    "\n",
    "     # Every time we see a \"Head\" joint, assume a new frame begins\n",
    "    for _, row in df.iterrows():\n",
    "        if row['Joint'] == 'Head' and current:\n",
    "            frames.append(pd.DataFrame(current))\n",
    "            current = []\n",
    "        current.append(row)\n",
    "    if current:\n",
    "        frames.append(pd.DataFrame(current))\n",
    "\n",
    "    # Determine the joint order from the first valid frame\n",
    "    joint_order = None\n",
    "    for fr in frames:\n",
    "        if 'Joint' in fr and fr['Joint'].nunique() >= 10:\n",
    "            joint_order = list(fr['Joint'])\n",
    "            break\n",
    "    if joint_order is None:\n",
    "        return None\n",
    "\n",
    "     # Create a 3D array: time × joints × coordinates\n",
    "    T = len(frames)\n",
    "    J = len(joint_order)\n",
    "    seq = np.full((T, J, 3), np.nan, dtype=float)\n",
    "\n",
    "     # Fill sequence array with x,y,z coordinates for each joint per frame\n",
    "    for t, fr in enumerate(frames):\n",
    "        # Map each joint name → (x, y, z)\n",
    "        name_to_xyz = {n: (x, y, z) for n, x, y, z in fr[['Joint', 'x', 'y', 'z']].itertuples(index=False)}\n",
    "        for j, name in enumerate(joint_order):\n",
    "            if name in name_to_xyz:\n",
    "                seq[t, j, :] = name_to_xyz[name]\n",
    "    return seq, joint_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4786e06a-44f9-4939-aae9-c7fd278fa86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 4. PREPROCESS EACH SEQUENCE\n",
    "# -------------------------------------------------------------\n",
    "# To feed into ML, all sequences must have:\n",
    "# - the same number of frames (resampling)\n",
    "# - comparable scale (normalization)\n",
    "# - dynamic info (velocities)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def resample_sequence(seq: np.ndarray, target_T: int = 60) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resample sequence to a fixed number of frames (default 60)\n",
    "    by linear interpolation along the time dimension.\n",
    "    \"\"\"\n",
    "    T, J, C = seq.shape\n",
    "    src_t = np.arange(T)\n",
    "    dst_t = np.linspace(0, T - 1, target_T)\n",
    "    out = np.empty((target_T, J, C), dtype=float)\n",
    "\n",
    "    for j in range(J):\n",
    "        for c in range(C):\n",
    "            # Fill any missing (NaN) data before interpolation\n",
    "            y = pd.Series(seq[:, j, c]).ffill().bfill().to_numpy()\n",
    "            out[:, j, c] = np.interp(dst_t, src_t, y)\n",
    "    return out\n",
    "\n",
    "\n",
    "def root_center_normalize(seq: np.ndarray, joint_names: list, root_name: str = 'Torso') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center coordinates around the torso joint and scale\n",
    "    by the median head-to-torso distance.\n",
    "    \"\"\"\n",
    "    T, J, C = seq.shape\n",
    "    # identify root and head joints (fallback to 0 if missing)\n",
    "    root_idx = joint_names.index(root_name) if root_name in joint_names else 0\n",
    "    head_idx = joint_names.index('Head') if 'Head' in joint_names else 0\n",
    "\n",
    "    # Center by subtracting torso coordinates\n",
    "    centered = seq - seq[:, [root_idx], :]\n",
    "\n",
    "    # Compute typical human scale factor\n",
    "    d = np.linalg.norm(centered[:, head_idx, :] - centered[:, root_idx, :], axis=1)\n",
    "    scale = np.median(d) if np.isfinite(np.median(d)) and np.median(d) != 0 else 1.0\n",
    "\n",
    "    return centered / scale\n",
    "\n",
    "\n",
    "def make_features(seq: np.ndarray, include_velocity: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert 3D sequence into a 1D feature vector:\n",
    "    - positions flattened across all frames and joints\n",
    "    - optional velocities (differences between consecutive frames)\n",
    "    \"\"\"\n",
    "    pos_flat = seq.reshape(-1)\n",
    "    feats = [pos_flat]\n",
    "\n",
    "    if include_velocity:\n",
    "        vel = np.diff(seq, axis=0)\n",
    "        feats.append(vel.reshape(-1))\n",
    "\n",
    "    return np.concatenate(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf81633-1600-4319-96cf-f5d07f687448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (180, 4998)\n",
      "Labels shape: (180,)\n",
      "Unique labels: ['a01' 'a02' 'a03' 'a04' 'a05' 'a06' 'a07' 'a08' 'a09' 'a10' 'a11' 'a12'\n",
      " 'a13' 'a14' 'a15' 'a16' 'a17' 'a18']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 5. BUILD FEATURE MATRIX X AND LABEL VECTOR y\n",
    "# -------------------------------------------------------------\n",
    "# For each CSV:\n",
    "#   - Parse into sequence\n",
    "#   - Resample to fixed 60 frames\n",
    "#   - Normalize by torso & head distance\n",
    "#   - Extract flattened features\n",
    "#   - Derive label from filename prefix (\"aXX\")\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for f in files:\n",
    "    parsed = parse_sequence(f)\n",
    "    if parsed is None:\n",
    "        continue\n",
    "    seq, joint_names = parsed\n",
    "\n",
    "    # Step 1: standardize frame length\n",
    "    seq = resample_sequence(seq, target_T=60)\n",
    "\n",
    "    # Step 2: normalize to remove global offset/scale\n",
    "    seq = root_center_normalize(seq, joint_names)\n",
    "\n",
    "    # Step 3: extract flattened features (positions + velocities)\n",
    "    feats = make_features(seq, include_velocity=True)\n",
    "    X.append(feats)\n",
    "\n",
    "    # Step 4: extract action label from file name\n",
    "    m = re.search(r'a(\\d+)_', os.path.basename(f))\n",
    "    label = f\"a{m.group(1)}\" if m else \"unknown\"\n",
    "    y.append(label)\n",
    "\n",
    "# Convert lists into NumPy arrays\n",
    "X = np.stack(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "print(\"Unique labels:\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7f61e5-3cbe-4556-b7cc-046050c503dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (144, 4998), Test set: (36, 4998)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 6. LABEL ENCODING AND DATA SPLIT\n",
    "# -------------------------------------------------------------\n",
    "# Machine learning models require numerical labels.\n",
    "# LabelEncoder maps each motion type string (a01, a02, ...)\n",
    "# into an integer ID. We then split data 80/20 for training/testing.\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "# Split data, ensuring each class is proportionally represented\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964991b1-ce6b-4cf9-b6a0-3c0c4c55cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature model trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 7. MODEL 1: FULL FEATURE RANDOM FOREST\n",
    "# -------------------------------------------------------------\n",
    "# - Scales each feature to zero-mean/unit-variance\n",
    "# - Trains a RandomForestClassifier on the full feature set\n",
    "#   (positions + velocities, no dimensionality reduction)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "clf_full = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True)),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=300,   # number of trees\n",
    "        max_depth=None,     # let trees grow fully\n",
    "        n_jobs=-1,          # use all CPU cores\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_full.fit(X_train, y_train)\n",
    "print(\"Full feature model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e20cf82-cfd2-4b3a-b94b-90fdfb14ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced feature model trained (PCA with 100 components).\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 8. MODEL 2: REDUCED FEATURE SET WITH PCA\n",
    "# -------------------------------------------------------------\n",
    "# PCA = Principal Component Analysis\n",
    "# Reduces dimensionality by keeping only top components\n",
    "# that explain most of the variance in the dataset.\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Number of principal components must be ≤ min(samples-1, features)\n",
    "n_components = min(100, X_train.shape[0] - 1, X_train.shape[1])\n",
    "\n",
    "clf_reduced = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True)),\n",
    "    ('pca', PCA(n_components=n_components, random_state=42)),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_reduced.fit(X_train, y_train)\n",
    "print(f\"Reduced feature model trained (PCA with {n_components} components).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7401e89-0511-4242-bc33-395568d0bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully in 'models/' folder.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 9. SAVE TRAINED MODELS\n",
    "# -------------------------------------------------------------\n",
    "# Following assignment instructions, we save:\n",
    "#   - Full feature model  -> finalized_model_M1.sav\n",
    "#   - Reduced feature model -> finalized_model_M2.sav\n",
    "#   - Label encoder -> label_encoder.sav\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "pickle.dump(clf_full, open(\"models/finalized_model_M1.sav\", \"wb\"))\n",
    "pickle.dump(clf_reduced, open(\"models/finalized_model_M2.sav\", \"wb\"))\n",
    "pickle.dump(le, open(\"models/label_encoder.sav\", \"wb\"))\n",
    "\n",
    "print(\"Models saved successfully in 'models/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c4998c2-36d0-4e0b-b6a9-cbb2db931b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model accuracy (sanity check): 0.278\n",
      "Reduced model accuracy (sanity check): 0.139\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 10. VERIFY LOADING USING ASSIGNMENT SNIPPET\n",
    "# -------------------------------------------------------------\n",
    "# Demonstrates the .sav files can be reloaded and used\n",
    "# directly to compute a quick test score.\n",
    "# (No formal performance analysis yet — next submission)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Load both models\n",
    "loaded_full = pickle.load(open(\"models/finalized_model_M1.sav\", \"rb\"))\n",
    "loaded_reduced = pickle.load(open(\"models/finalized_model_M2.sav\", \"rb\"))\n",
    "\n",
    "# Evaluate accuracy using .score() (built-in to scikit-learn)\n",
    "score_full = loaded_full.score(X_test, y_test)\n",
    "score_reduced = loaded_reduced.score(X_test, y_test)\n",
    "\n",
    "print(\"Full model accuracy (sanity check):\", round(score_full, 3))\n",
    "print(\"Reduced model accuracy (sanity check):\", round(score_reduced, 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
